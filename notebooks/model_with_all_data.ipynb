{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3821936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Useful function for dealing with data, and classical machine learning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helpers import (read_data,  # noqa E402\n",
    "                     create_segments_and_labels,\n",
    "                     save_converted_model,\n",
    "                     compare,\n",
    "                     plot_activity)\n",
    "\n",
    "# Set some standard parameters upfront\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "sns.set()  # Default seaborn look and feel\n",
    "plt.style.use('ggplot')\n",
    "print('keras version ', keras.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_data('../data/data_adem.txt')\n",
    "df1['user-id'] = 34\n",
    "df2 = read_data('../data/data_mathis.txt')\n",
    "df2 = df2[df2['activity'] != 'Jogging']\n",
    "df2['user-id'] = 35\n",
    "df3 = read_data('../data/WISDM.txt', ['user-id',\n",
    "                                      'activity',\n",
    "                                      'timestamp',\n",
    "                                      'x-axis',\n",
    "                                      'y-axis',\n",
    "                                      'z-axis'])\n",
    "df = pd.concat([df1, df2, df3])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare walking data\n",
    "df1W = df1[df1[\"activity\"] == 'Walking'][:1000]\n",
    "df2W = df2[df2[\"activity\"] == 'Walking'][:1000]\n",
    "df3W = df3[(df3[\"activity\"] == 'Walking') & (df3[\"user-id\"] == 20)][:1000]\n",
    "plot_activity(\"Walking\", df2W)\n",
    "plot_activity(\"Walking\", df2W)\n",
    "plot_activity(\"Walking\", df3W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e47f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare walking data\n",
    "df1W = df1[df1[\"activity\"] == 'Downstairs'][:1000]\n",
    "df2W = df2[df2[\"activity\"] == 'Downstairs'][:1000]\n",
    "df3W = df3[(df3[\"activity\"] == 'Downstairs') & (df3[\"user-id\"] == 20)][:1000]\n",
    "plot_activity(\"Downstairs\", df2W)\n",
    "plot_activity(\"Downstairs\", df2W)\n",
    "plot_activity(\"Downstairs\", df3W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 80\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824058f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from data\n",
    "LABELS = df[\"activity\"].unique()\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbf106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define column name of the label vector\n",
    "LABEL = \"ActivityEncoded\"\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df[LABEL] = le.fit_transform(df[\"activity\"].values.ravel())\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9756082c",
   "metadata": {},
   "source": [
    "first model: data schuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_segments_and_labels(\n",
    "    df,\n",
    "    TIME_PERIODS,\n",
    "    STEP_DISTANCE,\n",
    "    LABEL)\n",
    "x, y = shuffle(x, y)\n",
    "\n",
    "x_train, x_test = x[:int(0.8 * len(x))], x[int(0.8 * len(x)):]\n",
    "y_train, y_test = y[:int(0.8 * len(y))], y[int(0.8 * len(y)):]\n",
    "y_train_one_hot = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cell is used if we want to train the model with\n",
    "# new randomized weights, set the boolean below to True to do so\n",
    "use_new_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e677a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_new_weights:\n",
    "    num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "    num_classes = le.classes_.size\n",
    "    input_shape = (TIME_PERIODS, num_sensors)\n",
    "\n",
    "    print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "    # 1D CNN neural network\n",
    "    norm_layer = Normalization()\n",
    "    norm_layer.adapt(x_train)\n",
    "\n",
    "    model_m = Sequential()\n",
    "    # model_m.add(norm_layer)\n",
    "    model_m.add(Conv1D(100, 10, activation='relu', input_shape=input_shape))\n",
    "    model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "    model_m.add(MaxPooling1D(3))\n",
    "    model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "    # model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "    model_m.add(GlobalAveragePooling1D())\n",
    "    model_m.add(Dropout(0.5))\n",
    "    model_m.add(Dense(num_classes, activation='softmax'))\n",
    "    print(model_m.summary())\n",
    "\n",
    "    # The EarlyStopping callback monitors training accuracy:\n",
    "    # if it fails to improve for two consecutive epochs,\n",
    "    # training stops early\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "            monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
    "    ]\n",
    "\n",
    "    model_m.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50a745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set class weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(y_train),\n",
    "                                                  y_train)\n",
    "class_weights\n",
    "dict_weights = {}\n",
    "for i in range(len(class_weights)):\n",
    "    dict_weights[i] = class_weights[i]\n",
    "dict_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c37d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
    "]\n",
    "\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train_one_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1,\n",
    "                      class_weight=dict_weights)\n",
    "# summarize history for accuracy and loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['accuracy'], \"g--\",\n",
    "         label=\"Accuracy of training data\")\n",
    "plt.plot(history.history['val_accuracy'], \"g\",\n",
    "         label=\"Accuracy of validation data\")\n",
    "plt.plot(history.history['loss'], \"r--\",\n",
    "         label=\"Loss of training data\")\n",
    "plt.plot(history.history['val_loss'], \"r\",\n",
    "         label=\"Loss of validation data\")\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.ylabel('Accuracy and Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model_m.predict(x_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "cf_matrix = pd.DataFrame(confusion_matrix(y_test, max_y_pred_test),\n",
    "                         columns=LABELS, index=LABELS)\n",
    "sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%',\n",
    "            cmap='Greens')\n",
    "\n",
    "print(classification_report(y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9467a34f",
   "metadata": {},
   "source": [
    "second model: split data using user-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['user-id'] > 10]\n",
    "df_test = df[df['user-id'] <= 10]\n",
    "x_train, y_train = create_segments_and_labels(df_train,\n",
    "                                              TIME_PERIODS,\n",
    "                                              STEP_DISTANCE,\n",
    "                                              LABEL)\n",
    "y_train_one_hot = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set class weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(y_train),\n",
    "                                                  y_train)\n",
    "class_weights\n",
    "dict_weights = {}\n",
    "for i in range(len(class_weights)):\n",
    "    dict_weights[i] = class_weights[i]\n",
    "dict_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_new_weights:\n",
    "    num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "    num_classes = le.classes_.size\n",
    "    input_shape = (TIME_PERIODS, num_sensors)\n",
    "\n",
    "    print(\"\\n--- Create neural network model ---\\n\")\n",
    "\n",
    "    # 1D CNN neural network\n",
    "    norm_layer = Normalization()\n",
    "    norm_layer.adapt(x_train)\n",
    "\n",
    "    model_m = Sequential()\n",
    "    # model_m.add(norm_layer)\n",
    "    model_m.add(Conv1D(100, 10, activation='relu', input_shape=input_shape))\n",
    "    model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "    model_m.add(MaxPooling1D(3))\n",
    "    model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "    # model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "    model_m.add(GlobalAveragePooling1D())\n",
    "    model_m.add(Dropout(0.5))\n",
    "    model_m.add(Dense(num_classes, activation='softmax'))\n",
    "    print(model_m.summary())\n",
    "\n",
    "    # The EarlyStopping callback monitors training accuracy:\n",
    "    # if it fails to improve for two consecutive epochs,\n",
    "    # training stops early\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "            monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
    "    ]\n",
    "\n",
    "    model_m.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f476b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
    "]\n",
    "\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train_one_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1,\n",
    "                      class_weight=dict_weights)\n",
    "# summarize history for accuracy and loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['accuracy'], \"g--\",\n",
    "         label=\"Accuracy of training data\")\n",
    "plt.plot(history.history['val_accuracy'], \"g\",\n",
    "         label=\"Accuracy of validation data\")\n",
    "plt.plot(history.history['loss'], \"r--\",\n",
    "         label=\"Loss of training data\")\n",
    "plt.plot(history.history['val_loss'], \"r\",\n",
    "         label=\"Loss of validation data\")\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.ylabel('Accuracy and Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model_m.predict(x_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "cf_matrix = pd.DataFrame(confusion_matrix(y_test, max_y_pred_test),\n",
    "                         columns=LABELS, index=LABELS)\n",
    "sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%',\n",
    "            cmap='Greens')\n",
    "\n",
    "print(classification_report(y_test, max_y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
